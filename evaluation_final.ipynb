{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "oHD0XGDFo3RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Vt0nzSO0qKtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение результатов работы модели сущностей на тестовых данных"
      ],
      "metadata": {
        "id": "BS8lMsDro0ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    'B-Food': 0, 'B-Interior': 1, 'B-Price': 2, 'B-Service': 3,\n",
        "    'B-Whole': 4, 'I-Food': 5, 'I-Interior': 6, 'I-Price': 7,\n",
        "    'I-Service': 8, 'I-Whole': 9, 'O': 10}\n",
        "\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "def get_entities(text_id, text):\n",
        "    result = classifier(text)\n",
        "    to_return = []\n",
        "    words = []\n",
        "    entities = []\n",
        "    prev_entity = ''\n",
        "    positions = []\n",
        "\n",
        "    for i, token in enumerate(result):\n",
        "        entity_id = int(token['entity'].split('_')[-1])\n",
        "        entity = id2label[entity_id]\n",
        "        start_pos = token['start']\n",
        "        end_pos = token['end']\n",
        "\n",
        "        if token['word'].startswith('##'):\n",
        "            words[-1] += token['word'][2:]\n",
        "            positions[-1][1] = end_pos\n",
        "\n",
        "        elif entity.startswith('I') and entities:\n",
        "            words[-1] += f' {token[\"word\"]}'\n",
        "            positions[-1][1] = end_pos\n",
        "\n",
        "        else:\n",
        "            words.append(token['word'])\n",
        "            entities.append(entity)\n",
        "            positions.append([start_pos, end_pos])\n",
        "\n",
        "        prev_entity = entity\n",
        "\n",
        "    for word, entity, position in zip(words, entities, positions):\n",
        "        if entity != 'O':\n",
        "            to_return.append([str(text_id), entity[2:], word, str(position[0]), str(position[1]), 'neutral'])\n",
        "\n",
        "    return to_return"
      ],
      "metadata": {
        "id": "KSmHsUiao8MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"ner\", model=\"/content/drive/MyDrive/NLP Course/bert_wo_crf_deepvk_uncased_1e02\")"
      ],
      "metadata": {
        "id": "Svq5ksSlovJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Чтение тестовых данных и получение предсказаний"
      ],
      "metadata": {
        "id": "ZlbS-jRVgmgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_reviews = \"/content/drive/MyDrive/NLP Course/Data/test/test_reviews.txt\"\n",
        "data = pd.read_csv(dev_reviews, sep='\\t', names=['idx', 'text'])"
      ],
      "metadata": {
        "id": "5qRrHLvrpuXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "all_results = []\n",
        "for i, row in tqdm(data.iterrows()):\n",
        "    all_results.extend(get_entities(row['idx'], row['text']))\n"
      ],
      "metadata": {
        "id": "5nRwM02KqdAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_save = '\\n'.join(['\\t'.join(result) for result in all_results])"
      ],
      "metadata": {
        "id": "n0YwXm5rrGp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP Course/dev_pred_aspects.txt', 'w') as f:\n",
        "    f.write(to_save)"
      ],
      "metadata": {
        "id": "R4B3MVrrsVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предсказание тональности аспектов"
      ],
      "metadata": {
        "id": "EJCO8ITTg6bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для извлечения контекста вокруг аспекта с учетом целых слов\n",
        "def extract_context_with_category(text, start, end, category, window=50):\n",
        "    # Находим начало контекста и пробел назад от начальной позиции аспекта\n",
        "    start_idx = max(0, start - window)\n",
        "    while start_idx > 0 and text[start_idx] != ' ':\n",
        "        start_idx -= 1\n",
        "\n",
        "    # Находим конец контекста и пробел вперед от конечной позиции аспекта\n",
        "    end_idx = min(len(text), end + window)\n",
        "    while end_idx < len(text) and text[end_idx] != ' ':\n",
        "        end_idx += 1\n",
        "\n",
        "    context = text[start_idx:end_idx].strip()\n",
        "    return category + \" \" + context  # Добавление категории аспекта"
      ],
      "metadata": {
        "id": "bvZyNrWC3iDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {'negative': 0, 'neutral': 1, 'positive': 2, 'both': 3}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_path = \"/content/drive/MyDrive/NLP Course/bert_model.pth\"\n",
        "model_name = 'DeepPavlov/rubert-base-cased'\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_dict))\n",
        "\n",
        "# Загрузка сохраненных весов\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "blXF-Auc6rFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_aspects = pd.read_csv('/content/drive/MyDrive/NLP Course/dev_pred_aspects.txt', sep='\\t', names=['review_id', 'aspect_category', 'aspect_text', 'start_pos', 'end_pos', 'sentiment'])\n",
        "dev_reviews = pd.read_csv('/content/drive/MyDrive/NLP Course/Data/test/test_reviews.txt', sep='\\t', names=['review_id', 'review_text'])\n",
        "\n",
        "# Объединение данных\n",
        "merged_data = pd.merge(dev_aspects, dev_reviews, on='review_id')\n",
        "\n",
        "# Добавление контекста к данным\n",
        "merged_data['context'] = merged_data.apply(lambda row: extract_context_with_category(row['review_text'], row['start_pos'], row['end_pos'], row['aspect_category']), axis=1)"
      ],
      "metadata": {
        "id": "vgriZApuirDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_name, model_max_length=512)\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    encoded_input = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=64,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = encoded_input['input_ids'].to(device)\n",
        "    attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # Возвращаем тональность аспекта\n",
        "    return list(label_dict.keys())[list(label_dict.values()).index(prediction)]\n",
        "\n",
        "# Применение модели к каждому контексту и обновление значения тональности\n",
        "dev_aspects['sentiment'] = merged_data['context'].apply(predict_sentiment)\n",
        "\n",
        "dev_aspects.to_csv('dev_pred_aspects.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "hoWdeK8IiYG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_aspects.to_csv('/content/drive/MyDrive/NLP Course/dev_pred_aspects.txt', sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "A-gNqrOJ7nGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценивание"
      ],
      "metadata": {
        "id": "iGho1LAKoyrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voV6RarIoccZ"
      },
      "outputs": [],
      "source": [
        "gold_test_path = \"/content/drive/MyDrive/NLP Course/dev_aspects.txt\"\n",
        "pred_test_path = \"/content/drive/MyDrive/NLP Course/dev_pred_aspects.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "gold_aspect_cats = {}\n",
        "with open(gold_test_path) as fg:\n",
        "    for line in fg:\n",
        "        line = line.rstrip('\\r\\n').split('\\t')\n",
        "        if line[0] not in gold_aspect_cats:\n",
        "            gold_aspect_cats[line[0]] = {\"starts\":[], \"ends\":[], \"cats\":[], \"sents\":[]}\n",
        "        gold_aspect_cats[line[0]][\"starts\"].append(int(line[3]))\n",
        "        gold_aspect_cats[line[0]][\"ends\"].append(int(line[4]))\n",
        "        gold_aspect_cats[line[0]][\"cats\"].append(line[1])\n",
        "        gold_aspect_cats[line[0]][\"sents\"].append(line[5])"
      ],
      "metadata": {
        "id": "nOFaAuVHokoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_match, partial_match, full_cat_match, partial_cat_match = 0, 0, 0, 0\n",
        "total = 0\n",
        "fully_matched_pairs = []\n",
        "partially_matched_pairs = []\n",
        "with open(pred_test_path) as fp:\n",
        "    for line in fp:\n",
        "        total += 1\n",
        "        line = line.rstrip('\\r\\n').split('\\t')\n",
        "        start, end = int(line[3]), int(line[4])\n",
        "        category = line[1]\n",
        "        doc_gold_aspect_cats = gold_aspect_cats[line[0]]\n",
        "        if start in doc_gold_aspect_cats[\"starts\"]:\n",
        "            i = doc_gold_aspect_cats[\"starts\"].index(start)\n",
        "            if doc_gold_aspect_cats[\"ends\"][i] == end:\n",
        "                full_match += 1\n",
        "                if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                    full_cat_match += 1\n",
        "                else:\n",
        "                    partial_cat_match += 1\n",
        "                fully_matched_pairs.append(\n",
        "                    (\n",
        "                        [\n",
        "                            doc_gold_aspect_cats[\"starts\"][i],\n",
        "                            doc_gold_aspect_cats[\"ends\"][i],\n",
        "                            doc_gold_aspect_cats[\"cats\"][i],\n",
        "                            doc_gold_aspect_cats[\"sents\"][i]\n",
        "                        ],\n",
        "                        line\n",
        "                    )\n",
        "                )\n",
        "                continue\n",
        "        for s_pos in doc_gold_aspect_cats[\"starts\"]:\n",
        "            if start <= s_pos:\n",
        "                i = doc_gold_aspect_cats[\"starts\"].index(s_pos)\n",
        "                if doc_gold_aspect_cats[\"ends\"][i] == end:\n",
        "                    partial_match += 1\n",
        "                    partially_matched_pairs.append(\n",
        "                        (\n",
        "                            [\n",
        "                                doc_gold_aspect_cats[\"starts\"][i],\n",
        "                                doc_gold_aspect_cats[\"ends\"][i],\n",
        "                                doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                doc_gold_aspect_cats[\"sents\"][i]\n",
        "                            ],\n",
        "                            line\n",
        "                        )\n",
        "                    )\n",
        "                    if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                        partial_cat_match += 1\n",
        "                    continue\n",
        "                matched = False\n",
        "                for e_pos in doc_gold_aspect_cats[\"ends\"][i:]:\n",
        "                    if s_pos <= end <= e_pos:\n",
        "                        partial_match += 1\n",
        "                        partially_matched_pairs.append(\n",
        "                            (\n",
        "                                [\n",
        "                                    doc_gold_aspect_cats[\"starts\"][i],\n",
        "                                    doc_gold_aspect_cats[\"ends\"][i],\n",
        "                                    doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                    doc_gold_aspect_cats[\"sents\"][i]\n",
        "                                ],\n",
        "                                line\n",
        "                            )\n",
        "                        )\n",
        "                        if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                            partial_cat_match += 1\n",
        "                        matched = True\n",
        "                        break\n",
        "                if matched:\n",
        "                    break\n",
        "            if start > s_pos:\n",
        "                i = doc_gold_aspect_cats[\"starts\"].index(s_pos)\n",
        "                if start < doc_gold_aspect_cats[\"ends\"][i] <= end:\n",
        "                    partial_match += 1\n",
        "                    partially_matched_pairs.append(\n",
        "                        (\n",
        "                            [\n",
        "                                doc_gold_aspect_cats[\"starts\"][i],\n",
        "                                doc_gold_aspect_cats[\"ends\"][i],\n",
        "                                doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                doc_gold_aspect_cats[\"sents\"][i]\n",
        "                            ],\n",
        "                            line\n",
        "                        )\n",
        "                    )\n",
        "                    if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                        partial_cat_match += 1\n",
        "                    break"
      ],
      "metadata": {
        "id": "n-vQASV9oqyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метрики по сущностям"
      ],
      "metadata": {
        "id": "TXmneq4Z8P_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_size = sum([len(gold_aspect_cats[x][\"cats\"]) for x in gold_aspect_cats])\n",
        "print(f\"\"\"\n",
        "Full match precision: {full_match / total}\n",
        "Full match recall: {full_match / gold_size}\n",
        "Partial match ratio in pred: {(full_match + partial_match)  / total}\n",
        "Full category accuracy: {full_cat_match / total}\n",
        "Partial category accuracy: {(full_cat_match + partial_cat_match) / total}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "fK-QIsuwu_uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метрики по тональностям аспектов"
      ],
      "metadata": {
        "id": "pVOED5Hi8EQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_accuracy(matches):\n",
        "    matched_sentiment = 0.\n",
        "    for pair in matches:\n",
        "        *_, gold_s = pair[0]\n",
        "        *_, pred_s = pair[1]\n",
        "        if gold_s == pred_s:\n",
        "            matched_sentiment += 1\n",
        "    print(f\"Mention sentiment accuracy: {matched_sentiment / len(matches)}\")"
      ],
      "metadata": {
        "id": "eVRs3SD06K1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_accuracy(fully_matched_pairs)"
      ],
      "metadata": {
        "id": "JXsVe64J8EzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_accuracy(partially_matched_pairs)"
      ],
      "metadata": {
        "id": "wLvPrX1F8HQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}